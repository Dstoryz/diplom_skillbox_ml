{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1QnC7lV7oVFk5OZCm75fqbLAfD9qBy9bw","timestamp":1674708114951}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jzRKNe1iSlNW"},"source":["# Google Colab: Access Webcam for Images and Video\n","This notebook will go through how to access and run code on images and video taken using your webcam.  \n","\n","For this purpose of this tutorial we will be using OpenCV's Haar Cascade to do face detection on our Webcam image and video."]},{"cell_type":"code","metadata":{"id":"Fj9YcAnsT4B_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e509a957-ef6c-498e-d508-13fb7b73130a","executionInfo":{"status":"ok","timestamp":1680440299473,"user_tz":-180,"elapsed":11699,"user":{"displayName":"Александр Гомелев","userId":"16740658847247968809"}}},"source":["# import dependencies\n","from IPython.display import display, Javascript, Image\n","from google.colab.output import eval_js\n","from base64 import b64decode, b64encode\n","import cv2\n","import numpy as np\n","import PIL\n","import io\n","import html\n","import time\n","!pip install face_recognition\n","import face_recognition\n","\n","import tensorflow as tf\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","\n","from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, \\\n","Dropout, BatchNormalization\n","\n","import pathlib\n","from pathlib import Path\n","from google.colab import drive\n","drive.mount('/content/drive')\n","from tensorflow.keras.applications.vgg19 import preprocess_input\n","\n","!pip install ultralytics\n","from ultralytics import YOLO\n","\n"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: face_recognition in /usr/local/lib/python3.9/dist-packages (1.3.0)\n","Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.9/dist-packages (from face_recognition) (8.1.3)\n","Requirement already satisfied: face-recognition-models>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from face_recognition) (0.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from face_recognition) (1.22.4)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from face_recognition) (8.4.0)\n","Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.9/dist-packages (from face_recognition) (19.24.1)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: ultralytics in /usr/local/lib/python3.9/dist-packages (8.0.59)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (8.4.0)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (6.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from ultralytics) (5.9.4)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (4.7.0.72)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (1.10.1)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (0.12.2)\n","Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (0.1.1.post2209072238)\n","Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (0.14.1+cu116)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (2.27.1)\n","Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (1.22.4)\n","Requirement already satisfied: sentry-sdk in /usr/local/lib/python3.9/dist-packages (from ultralytics) (1.18.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (1.4.4)\n","Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (4.65.0)\n","Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (1.13.1+cu116)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.4.4)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (3.0.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (23.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.0.7)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (4.39.3)\n","Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (5.12.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (0.11.0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.4->ultralytics) (2022.7.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->ultralytics) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->ultralytics) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->ultralytics) (1.26.15)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->ultralytics) (4.5.0)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3.2.2->ultralytics) (3.15.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->ultralytics) (1.16.0)\n"]}]},{"cell_type":"markdown","metadata":{"id":"L6pCmkJrUC9g"},"source":["## Helper Functions\n","Below are a few helper function to make converting between different image data types and formats. "]},{"cell_type":"code","metadata":{"id":"09b_0FAnUa9y","executionInfo":{"status":"ok","timestamp":1680440299476,"user_tz":-180,"elapsed":34,"user":{"displayName":"Александр Гомелев","userId":"16740658847247968809"}}},"source":["# function to convert the JavaScript object into an OpenCV image\n","def js_to_image(js_reply):\n","  \"\"\"\n","  Params:\n","          js_reply: JavaScript object containing image from webcam\n","  Returns:\n","          img: OpenCV BGR image\n","  \"\"\"\n","  # decode base64 image\n","  image_bytes = b64decode(js_reply.split(',')[1])\n","  # convert bytes to numpy array\n","  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n","  # decode numpy array into OpenCV BGR image\n","  img = cv2.imdecode(jpg_as_np, flags=1)\n","\n","  return img\n","\n","# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n","def bbox_to_bytes(bbox_array):\n","  \"\"\"\n","  Params:\n","          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n","  Returns:\n","        bytes: Base64 image byte string\n","  \"\"\"\n","  # convert array into PIL image\n","  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n","  iobuf = io.BytesIO()\n","  # format bbox into png for return\n","  bbox_PIL.save(iobuf, format='png')\n","  # format return string\n","  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n","\n","  return bbox_bytes"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MaZIOR4WaT64"},"source":["## Haar Cascade Classifier\n","For this tutorial we will run a simple object detection algorithm called Haar Cascade on our images and video fetched from our webcam. OpenCV has a pre-trained Haar Cascade face detection model. "]},{"cell_type":"code","metadata":{"id":"ZpA68lTrcvZs","executionInfo":{"status":"ok","timestamp":1680440299477,"user_tz":-180,"elapsed":30,"user":{"displayName":"Александр Гомелев","userId":"16740658847247968809"}}},"source":["# initialize the Haar Cascade face detection model\n","# face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"ghUlAJzKSjFT","executionInfo":{"status":"ok","timestamp":1680440299478,"user_tz":-180,"elapsed":29,"user":{"displayName":"Александр Гомелев","userId":"16740658847247968809"}}},"source":["# JavaScript to properly create our live video stream using our webcam as input\n","def video_stream():\n","  js = Javascript('''\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","    \n","    var pendingResolve = null;\n","    var shutdown = false;\n","    \n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","    \n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","    \n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","      \n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","           \n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","      \n","      const instruction = document.createElement('div');\n","      instruction.innerHTML = \n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","      \n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","      \n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","      \n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","            \n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","      \n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","      \n","      return {'create': preShow - preCreate, \n","              'show': preCapture - preShow, \n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    ''')\n","\n","  display(js)\n","  \n","def video_frame(label, bbox):\n","  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n","  return data"],"execution_count":14,"outputs":[]},{"cell_type":"code","source":["path = Path(\"/content/drive/My Drive/diplom/model_save_ocv/checkpoint_best128_new_DS_24-02.h5\")"],"metadata":{"id":"2qc-SXGfUe1O","executionInfo":{"status":"ok","timestamp":1680440299479,"user_tz":-180,"elapsed":28,"user":{"displayName":"Александр Гомелев","userId":"16740658847247968809"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["%%capture --no-display\n","\n","class em_model:\n","    def __init__(self, config:str):\n","        self.config = config\n","        self.model = tf.keras.models.load_model(self.config)\n","        # warm-up model on a random sample\n","        self.img_size = 128\n","        sample = tf.random.uniform((1, self.img_size, self.img_size, 3), 0, 1)\n","        self.model(sample)\n","        self.class_names = {0: 'anger',\n","                      1: 'contempt',\n","                      2: 'disgust',\n","                      3: 'fear',\n","                      4: 'happy',\n","                      5: 'neutral',\n","                      6: 'sad',\n","                      7: 'surprise',\n","                      8: 'uncertain'}\n","        \n","    def predict_model(self, image):\n","        input_arr = cv2.resize(image, (128,128), interpolation=cv2.INTER_AREA)\n","        input_arr = preprocess_input(input_arr)\n","        input_arr = np.array([input_arr])  # Convert single image to a batch.\n","          \n","        predictions = self.model.predict(input_arr)\n","\n","        emotion = self.class_names[np.argmax(predictions)]\n","        return emotion"],"metadata":{"id":"2HVTNOYPDnqm","executionInfo":{"status":"ok","timestamp":1680440299480,"user_tz":-180,"elapsed":28,"user":{"displayName":"Александр Гомелев","userId":"16740658847247968809"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["model_emotion = em_model(path)"],"metadata":{"id":"-xEJxUwckHzv","executionInfo":{"status":"ok","timestamp":1680440301377,"user_tz":-180,"elapsed":1925,"user":{"displayName":"Александр Гомелев","userId":"16740658847247968809"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["!cp /content/drive/MyDrive/diplom/yolov8n-face.pt /content/"],"metadata":{"id":"NZNvhsGWxi5e","executionInfo":{"status":"ok","timestamp":1680440301379,"user_tz":-180,"elapsed":26,"user":{"displayName":"Александр Гомелев","userId":"16740658847247968809"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["model_path = '/content/yolov8n-face.pt'\n","model = YOLO(model_path)"],"metadata":{"id":"LBpmx83pyD_x","executionInfo":{"status":"ok","timestamp":1680440301380,"user_tz":-180,"elapsed":19,"user":{"displayName":"Александр Гомелев","userId":"16740658847247968809"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["%%capture --no-display\n","# start streaming video from webcam\n","video_stream()\n","# label for video\n","\n","label_html = 'Capturing...'\n","# initialze bounding box to empty\n","bbox = ''\n","count = 0 \n","frameSize = (640, 480) \n","out = cv2.VideoWriter('output_video.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 25, frameSize) \n","  # for filename in glob.glob('D:/images/*.jpg'): \n","  #   img = cv2.imread(filename) \n","  #   out.write(img) \n","  # out.release()\n","\n","while True:\n","    js_reply = video_frame(label_html, bbox)\n","    if not js_reply:\n","        break\n","    # convert JS response to OpenCV Image\n","    img = js_to_image(js_reply[\"img\"])\n","    # create transparent overlay for bounding box\n","    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n","    \n","    results = model.predict(source=img, show=False, save=False, conf=0.25, imgsz=640, line_thickness=3, max_det=1000)\n","    for result in results:\n","      img = result[0].orig_img\n","\n","      bxes = result[0].boxes\n","      if len(bxes.xywh) >= 1:\n","        for x, y, w, h in bxes.xyxy.tolist():\n","          x, y, w, h, = int(x), int(y), int(w), int(h)\n","          # print(int(y), int(h), int(x), int(w))\n","          img_box = img[int(y):int(h), int(x):int(w), :]\n","\n","          label = model_emotion.predict_model(img_box);\n","          box_array = cv2.rectangle(bbox_array,(x, y),(w, h),(255,0,0),2)\n","          # result[0].names[0] = label\n","          cv2.putText(bbox_array, label, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n","          # out.write(img) \n","\n","    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n","    \n","    # convert overlay of bbox into bytes\n","    bbox_bytes = bbox_to_bytes(bbox_array)\n","    # update bbox so next frame gets new overlay\n","    bbox = bbox_bytes\n","\n","    out.write(img) \n","out.release()"],"metadata":{"id":"oomRta4QsLs7","colab":{"base_uri":"https://localhost:8080/","height":330},"executionInfo":{"status":"ok","timestamp":1680440375193,"user_tz":-180,"elapsed":13477,"user":{"displayName":"Александр Гомелев","userId":"16740658847247968809"}},"outputId":"365bf646-4363-4589-9566-543a7dafec8a"},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","    \n","    var pendingResolve = null;\n","    var shutdown = false;\n","    \n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","    \n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","    \n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","      \n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","           \n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","      \n","      const instruction = document.createElement('div');\n","      instruction.innerHTML = \n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","      \n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","      \n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","      \n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","            \n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","      \n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","      \n","      return {'create': preShow - preCreate, \n","              'show': preCapture - preShow, \n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\n","0: 480x640 1 face, 16.5ms\n","Speed: 0.7ms preprocess, 16.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 480x640 1 face, 13.2ms\n","Speed: 0.6ms preprocess, 13.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 480x640 1 face, 16.9ms\n","Speed: 3.9ms preprocess, 16.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 480x640 1 face, 15.2ms\n","Speed: 0.6ms preprocess, 15.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 480x640 1 face, 25.2ms\n","Speed: 0.6ms preprocess, 25.2ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 480x640 1 face, 17.6ms\n","Speed: 0.6ms preprocess, 17.6ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n"]}]},{"cell_type":"code","source":["# Здесь все на камеру выводится, но не удалось вывести видео в отдельный экран или отключить вывод логов"],"metadata":{"id":"Uay8TolnAG8r"},"execution_count":null,"outputs":[]}]}